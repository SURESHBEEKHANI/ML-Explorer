{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyOfTdORQdMZBjMiHcpsEZDU",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/DeepLearning/blob/main/Autoencoder.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GjuXDV6atwE-"
      },
      "outputs": [],
      "source": [
        "# Import TensorFlow and its submodules\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.datasets import mnist\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Loading the dataset without Target Variables\n",
        "(x_train, _), (x_test, _) = mnist.load_data()"
      ],
      "metadata": {
        "id": "GDCpfRJowo8x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "n = 10  # how many digits we will display\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "# Loop through the first 'n' samples in the test dataset\n",
        "for i in range(n):\n",
        "    # display original\n",
        "    ax = plt.subplot(1, n, i + 1)  # Create a subplot for each image\n",
        "    plt.imshow(x_test[i].reshape(28, 28))  # Display the image (28x28 pixels)\n",
        "    plt.gray()  # Set the color map to grayscale\n",
        "    ax.get_xaxis().set_visible(False)  # Hide the x-axis\n",
        "    ax.get_yaxis().set_visible(False)  # Hide the y-axis\n",
        "\n",
        "plt.show()  # Display the grid of images\n",
        "plt.close()  # Close the plot window\n"
      ],
      "metadata": {
        "id": "18afEMBLwsoD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalize the pixel values of training and testing images by converting to float32 and scaling to [0.0, 1.0] range\n",
        "x_train = x_train.astype('float32') / 255.\n",
        "x_test = x_test.astype('float32') / 255.\n",
        "\n",
        "# Reshape the training and testing images from 2D (28x28) to 1D (784) for neural network input\n",
        "x_train = x_train.reshape((len(x_train), 28*28*1))\n",
        "x_test = x_test.reshape((len(x_test), 28*28*1))\n",
        "\n",
        "# Print the shapes of the training and testing data arrays for verification\n",
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "id": "CQJH30VAxmu4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary modules and layers from TensorFlow Keras\n",
        "from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, UpSampling2D, Flatten, Reshape\n",
        "from tensorflow.keras.models import Model\n"
      ],
      "metadata": {
        "id": "NnIZJPxayQHI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input layer for the autoencoder\n",
        "input_layer_cnv = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder\n",
        "# Convolutional layers for feature extraction\n",
        "ae_cnv_en = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\")(input_layer_cnv)\n",
        "ae_cnv_en = MaxPooling2D((2, 2), padding=\"same\")(ae_cnv_en)\n",
        "\n",
        "ae_cnv_en = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(ae_cnv_en)\n",
        "ae_cnv_en = MaxPooling2D((2, 2), padding=\"same\")(ae_cnv_en)\n",
        "\n",
        "ae_cnv_en = Conv2D(4, (3, 3), activation=\"relu\", padding=\"same\")(ae_cnv_en)\n",
        "ae_cnv_en = MaxPooling2D((2, 2), padding=\"same\")(ae_cnv_en)\n",
        "\n",
        "# Flatten layer for encoding\n",
        "ae_cnv_en = Flatten(name=\"bot\")(ae_cnv_en)\n",
        "\n",
        "# Decoder\n",
        "# Reshape the flattened output to 4x4x4\n",
        "ae_cnv_de = Reshape((4, 4, 4), input_shape=(64,), name=\"botnext0\")(ae_cnv_en)\n",
        "\n",
        "# Upsampling and convolutional layers for reconstruction\n",
        "ae_cnv_de = Conv2D(4, (3, 3), activation=\"relu\", padding=\"same\", name=\"botnext1\")(ae_cnv_de)\n",
        "ae_cnv_de = UpSampling2D((2, 2), name=\"botnext2\")(ae_cnv_de)\n",
        "\n",
        "ae_cnv_de = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", name=\"botnext3\")(ae_cnv_de)\n",
        "ae_cnv_de = UpSampling2D((2, 2), name=\"botnext4\")(ae_cnv_de)\n",
        "\n",
        "ae_cnv_de = Conv2D(32, (3, 3), activation=\"relu\", padding=\"valid\", name=\"botnext5\")(ae_cnv_de)\n",
        "ae_cnv_de = UpSampling2D((2, 2), name=\"botnext6\")(ae_cnv_de)\n",
        "\n",
        "# Final convolutional layer with sigmoid activation for image reconstruction\n",
        "ae_cnv_de = Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\", name=\"botnext7\")(ae_cnv_de)\n",
        "\n",
        "# Create the autoencoder model with the input and output layers\n",
        "Ae_Conv = Model(inputs=input_layer_cnv, outputs=ae_cnv_de)\n",
        "\n",
        "# Compile the model with SGD optimizer, binary cross-entropy loss, and accuracy metric\n",
        "Ae_Conv.compile(optimizer=tf.keras.optimizers.SGD(0.09, clipvalue=2.5), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "# Print a summary of the model architecture\n",
        "Ae_Conv.summary()\n"
      ],
      "metadata": {
        "id": "8ZXo5mv7y0YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an encoder model using the input layer and the encoder part of the autoencoder\n",
        "ae_conv_encoder = Model(inputs=input_layer_cnv, outputs=Ae_Conv.get_layer(\"bot\").output, name=\"Conv_AE_encoder\")\n",
        "\n",
        "# Print a summary of the encoder model architecture\n",
        "ae_conv_encoder.summary()\n"
      ],
      "metadata": {
        "id": "MRUuEu1z0ayF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input layer for the decoder with the shape (64,)\n",
        "encode_inp_cnv = Input(shape=(64,))\n",
        "\n",
        "# Initialize a temporary variable to hold the decoder's output\n",
        "tmp_dec = Ae_Conv.get_layer(\"botnext0\")(encode_inp_cnv)\n",
        "\n",
        "# Loop through the layers of the decoder\n",
        "for i in range(1, 8):\n",
        "    st = \"botnext{}\".format(i)\n",
        "    tmp_dec = Ae_Conv.get_layer(st)(tmp_dec)\n",
        "\n",
        "# Create the decoder model with the input and final decoder layer as outputs\n",
        "ae_conv_decoder = Model(inputs=encode_inp_cnv, outputs=tmp_dec, name=\"Conv_AE_decoder\")\n",
        "\n",
        "# Print a summary of the decoder model architecture\n",
        "ae_conv_decoder.summary()\n"
      ],
      "metadata": {
        "id": "z6s2uDDm0s7V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reshape the training dataset to its original format\n",
        "x_train = x_train.reshape(x_train.shape[0], 28, 28, 1)\n",
        "\n",
        "# Reshape the testing dataset to its original format\n",
        "x_test = x_test.reshape(x_test.shape[0], 28, 28, 1)\n",
        "\n",
        "# Print the shapes of the reshaped datasets\n",
        "print(x_train.shape, x_test.shape)\n"
      ],
      "metadata": {
        "id": "vrXccmkq1BGp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from time import time\n",
        "\n",
        "# Record the start time\n",
        "tic = time()\n",
        "\n",
        "# Train the autoencoder\n",
        "Ae_Conv.fit(x_train, x_train,\n",
        "            epochs=100,              # Number of training epochs\n",
        "            verbose=2,               # Verbosity level (0=silent, 1=progress bar, 2=one line per epoch)\n",
        "            batch_size=524,         # Batch size for training\n",
        "            shuffle=False,           # Don't shuffle the training data (assuming data is already shuffled)\n",
        "            validation_split=0.1)    # Percentage of training data used for validation\n",
        "\n",
        "# Record the end time\n",
        "toc = time()\n",
        "\n",
        "# Calculate and print the training time\n",
        "print(\"Training Took {} Secs\".format(toc - tic))\n"
      ],
      "metadata": {
        "id": "DawpAV-W10Gv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the autoencoder encoder to encode test images\n",
        "encoded_imgs = ae_conv_encoder.predict(x_test)\n",
        "\n",
        "# Print the shape of the encoded representations\n",
        "print(encoded_imgs.shape)\n"
      ],
      "metadata": {
        "id": "iYHKrHfZ5Hb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the autoencoder decoder to decode the encoded representations\n",
        "decoded_imgs = ae_conv_decoder.predict(encoded_imgs)\n",
        "\n",
        "# Generate reconstructed images directly from the autoencoder\n",
        "reconstructed_images = Ae_Conv.predict(x_test)\n",
        "\n",
        "# Print information about the shapes of the decoded and encoded representations\n",
        "print(\"Recreated image representation of shape {} using decoder and reduced image representation of shape {}\".format(decoded_imgs.shape, encoded_imgs.shape))\n",
        "\n",
        "# Define the number of digits to display\n",
        "n = 10  # how many digits we will display\n",
        "k = 12  # multiplier\n",
        "\n",
        "# Create a figure for displaying the original and reconstructed images\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "# Loop through the first 'n' images\n",
        "for i in range(n):\n",
        "    # Display the original image\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i * k].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display the reconstructed image\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_imgs[i * k].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "# Show the plot with original and reconstructed images\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "OGmIrWgw5pd3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Generate reconstructed images directly from the full autoencoder\n",
        "decoded_images = Ae_Conv.predict(x_test)\n",
        "\n",
        "# Print information about the shapes of the decoded images and the original images\n",
        "print(\"Recreated image representation of shape {} using Decoder and reduced Image representation of shape {}\".format(decoded_images.shape, x_test.shape))\n",
        "\n",
        "# Define the number of digits to display\n",
        "n = 10  # how many digits we will display\n",
        "k = 12  # multiplier\n",
        "\n",
        "# Create a figure for displaying the original and reconstructed images\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "# Loop through the first 'n' images\n",
        "for i in range(n):\n",
        "    # Display the original image\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test[i * k].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display the reconstructed image\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(decoded_images[i * k].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "# Show the plot with original and reconstructed images\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "lQ7jAiRo6C5l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(x_train.shape)\n",
        "print(x_test.shape)\n"
      ],
      "metadata": {
        "id": "p10RdDt_6bg6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the noise factor\n",
        "noise_factor = 0.5\n",
        "\n",
        "# Add noise to the training dataset\n",
        "x_train_noisy = x_train + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_train.shape)\n",
        "\n",
        "# Add noise to the testing dataset\n",
        "x_test_noisy = x_test + noise_factor * np.random.normal(loc=0.0, scale=1.0, size=x_test.shape)\n",
        "\n",
        "# Clip the noisy data to ensure values remain in the [0, 1] range\n",
        "x_train_noisy = np.clip(x_train_noisy, 0., 1.)\n",
        "x_test_noisy = np.clip(x_test_noisy, 0., 1.)\n",
        "\n",
        "# Print the shapes of the noisy datasets\n",
        "print(x_train_noisy.shape, x_test_noisy.shape)\n"
      ],
      "metadata": {
        "id": "gVt-YPn_64_j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the number of noisy digits to display\n",
        "n = 10\n",
        "\n",
        "# Create a figure for displaying the noisy images\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "# Loop through the first 'n' noisy training images\n",
        "for i in range(n):\n",
        "    # Display the original noisy image\n",
        "    ax = plt.subplot(1, n, i + 1)\n",
        "    plt.imshow(x_train_noisy[i].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "# Show the plot with noisy images\n",
        "plt.show()\n",
        "plt.close()  # Close the plot to free up resources\n"
      ],
      "metadata": {
        "id": "yjV_AG167aIG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the input layer with the shape (28, 28, 1)\n",
        "input_layer_cnv = Input(shape=(28, 28, 1))\n",
        "\n",
        "# Encoder layers\n",
        "ae_cnv_en = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", kernel_initializer=\"he_normal\")(input_layer_cnv)\n",
        "ae_cnv_en = MaxPooling2D((2, 2), padding=\"same\")(ae_cnv_en)\n",
        "\n",
        "ae_cnv_en = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\")(ae_cnv_en)\n",
        "ae_cnv_en = MaxPooling2D((2, 2), padding=\"same\")(ae_cnv_en)\n",
        "\n",
        "ae_cnv_en = Conv2D(4, (3, 3), activation=\"relu\", padding=\"same\")(ae_cnv_en)\n",
        "ae_cnv_en = MaxPooling2D((2, 2), padding=\"same\")(ae_cnv_en)\n",
        "\n",
        "# Flatten the output of the encoder\n",
        "ae_cnv_en = Flatten(name=\"bot\")(ae_cnv_en)\n",
        "\n",
        "# Decoder layers\n",
        "ae_cnv_de = Reshape((4, 4, 4), input_shape=(64,), name=\"botnext0\")(ae_cnv_en)\n",
        "ae_cnv_de = Conv2D(4, (3, 3), activation=\"relu\", padding=\"same\", name=\"botnext1\")(ae_cnv_de)\n",
        "ae_cnv_de = UpSampling2D((2, 2), name=\"botnext2\")(ae_cnv_de)\n",
        "\n",
        "ae_cnv_de = Conv2D(32, (3, 3), activation=\"relu\", padding=\"same\", name=\"botnext3\")(ae_cnv_de)\n",
        "ae_cnv_de = UpSampling2D((2, 2), name=\"botnext4\")(ae_cnv_de)\n",
        "\n",
        "ae_cnv_de = Conv2D(32, (3, 3), activation=\"relu\", padding=\"valid\", name=\"botnext5\")(ae_cnv_de)\n",
        "ae_cnv_de = UpSampling2D((2, 2), name=\"botnext6\")(ae_cnv_de)\n",
        "\n",
        "ae_cnv_de = Conv2D(1, (3, 3), activation=\"sigmoid\", padding=\"same\", name=\"botnext7\")(ae_cnv_de)\n",
        "\n",
        "# Create the denoising autoencoder model with input and output layers\n",
        "Ae_Conv_denoise = Model(inputs=input_layer_cnv, outputs=ae_cnv_de)\n",
        "\n",
        "# Compile the model with optimizer, loss function, and metrics\n",
        "Ae_Conv_denoise.compile(optimizer=tf.keras.optimizers.Adadelta(0.1, clipvalue=2), loss='binary_crossentropy', metrics=[\"accuracy\"])\n",
        "\n",
        "# Print a summary of the denoising autoencoder model architecture\n",
        "Ae_Conv_denoise.summary()\n"
      ],
      "metadata": {
        "id": "Y7KGBZ4g8OoY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an encoder model by extracting the encoder part from the denoising autoencoder\n",
        "ae_conv_dns_encoder = Model(inputs=input_layer_cnv, outputs=Ae_Conv_denoise.get_layer(\"bot\").output, name=\"Conv_AE_dns_encoder\")\n",
        "\n",
        "# Print a summary of the encoder model\n",
        "ae_conv_dns_encoder.summary()\n"
      ],
      "metadata": {
        "id": "ckufiPJY8z5I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Define an input layer for the decoder with the shape (64,)\n",
        "encode_inp_cnv = Input(shape=(64,))\n",
        "\n",
        "# Initialize a temporary variable to track the decoder layers\n",
        "tmp_dec = Ae_Conv_denoise.get_layer(\"botnext0\")(encode_inp_cnv)\n",
        "\n",
        "# Loop through the layers of the decoder (botnext1 to botnext7)\n",
        "for i in range(1, 8):\n",
        "    st = \"botnext{}\".format(i)\n",
        "    tmp_dec = Ae_Conv_denoise.get_layer(st)(tmp_dec)\n",
        "\n",
        "# Create the decoder model\n",
        "ae_conv_dns_decoder = Model(inputs=encode_inp_cnv, outputs=tmp_dec, name=\"Conv_AE_dns_decoder\")\n",
        "\n",
        "# Print a summary of the decoder model\n",
        "ae_conv_dns_decoder.summary()\n"
      ],
      "metadata": {
        "id": "iGLE6amk9MM6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Import the 'time' function from the 'time' module\n",
        "from time import time\n",
        "\n",
        "# Record the current time to measure training duration\n",
        "tic = time()\n",
        "\n",
        "# Train the denoising autoencoder\n",
        "Ae_Conv_denoise.fit(x_train_noisy, x_train,\n",
        "                    epochs=100,             # Number of training epochs\n",
        "                    verbose=2,              # Verbosity level (2 for more details)\n",
        "                    batch_size=256,         # Batch size for training\n",
        "                    shuffle=False,          # Don't shuffle the dataset (you may want to change this)\n",
        "                    validation_split=0.1)   # Percentage of data to use for validation\n",
        "\n",
        "# Record the time after training\n",
        "toc = time()\n",
        "\n",
        "# Calculate and print the training duration\n",
        "print(\"Training Took {} Secs\".format(toc - tic))\n"
      ],
      "metadata": {
        "id": "EW7r4TwD9xNL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the denoising autoencoder's encoder to obtain encoded representations\n",
        "dns_encoded_imgs = ae_conv_dns_encoder.predict(x_test_noisy)\n",
        "\n",
        "# Print the shape of the encoded representations\n",
        "print(dns_encoded_imgs.shape)\n"
      ],
      "metadata": {
        "id": "dUI-amqH-OBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reconstruct images using the denoising autoencoder's decoder\n",
        "dns_decoded_imgs = ae_conv_dns_decoder.predict(dns_encoded_imgs)\n",
        "\n",
        "# Print information about the shapes of the reconstructed images and the encoded representations\n",
        "print(\"Recreated image Representation of Shape {} using Decoder and reduced Image representation of shape {}\".format(dns_decoded_imgs.shape, dns_encoded_imgs.shape))\n",
        "\n",
        "# Define the number of noisy digits to display and a multiplier\n",
        "n = 10\n",
        "k = 16\n",
        "\n",
        "# Create a figure for displaying the original and reconstructed images\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "# Loop through the first 'n' noisy images\n",
        "for i in range(n):\n",
        "    # Display the original noisy image\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test_noisy[i * k].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display the reconstructed image\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(dns_decoded_imgs[i * k].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "# Show the plot with noisy and reconstructed images\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "HEPjdfpi-wa0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the denoising autoencoder's decoder to reconstruct images\n",
        "dns_decoded_imgs = ae_conv_dns_decoder.predict(dns_encoded_imgs)\n",
        "\n",
        "# Print information about the shapes of the reconstructed images and the encoded representations\n",
        "print(\"Recreated image Representation of Shape {} using Decoder and reduced Image representation of shape {}\".format(dns_decoded_imgs.shape, dns_encoded_imgs.shape))\n",
        "\n",
        "# Define the number of noisy digits to display and a multiplier\n",
        "n = 10\n",
        "k = 16\n",
        "\n",
        "# Create a figure for displaying the original and reconstructed images\n",
        "plt.figure(figsize=(20, 4))\n",
        "\n",
        "# Loop through the first 'n' noisy images\n",
        "for i in range(n):\n",
        "    # Display the original noisy image\n",
        "    ax = plt.subplot(2, n, i + 1)\n",
        "    plt.imshow(x_test_noisy[i * k].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "    # Display the reconstructed image\n",
        "    ax = plt.subplot(2, n, i + 1 + n)\n",
        "    plt.imshow(dns_decoded_imgs[i * k].reshape(28, 28))\n",
        "    plt.gray()\n",
        "    ax.get_xaxis().set_visible(False)\n",
        "    ax.get_yaxis().set_visible(False)\n",
        "\n",
        "# Show the plot with noisy and reconstructed images\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "FxeiZIKu_KbP"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}